# Model

## Summary

$$
P_{binding} = fn(sequence, smiles)
$$

This page describes the deep learning model constructed for this project.
The model is designed to estimate the likelihood of a binding interaction between a given Cytochrome P450 sequence and a ligand SMILES code.
The intended end uses of the model are:

1. Virtually screening sequences for potential activity with a given compound.
2. Optimally design an enzyme-ligand screening experiment.

## Aim

- [ ] Build and train a model that: 
	- [x] Takes input enzyme sequence and a compound SMILES and outputs an estimate of binding likelihood interaction between the two.
- [ ] Pre-train prototype on large sequence-smiles dataset and save weights.
	- [x] Set up GPU machine
	- [ ] Run for $n$ iterations, save weights
	- [ ] Run again with validation
	- [ ] Outputs uncertainty

## Approach

### Recommender Systems

Abstractly, the problem of predicting the binding likelihood between a one of $m$ proteins and one of $n$ small molecules can be likened to filling empty the values of an $n \times m$ matrix, where rows and columns refer to proteins and small molecules and values are the probability of a binding interaction between the two:


\begin{matrix} 
 & compound_i & compound_{i+1} & ... & compound_{i+n} \\
sequence_i  & P_{binding_{i,j}} &  &  & \\  
sequence_{i+1} &  & \ddots &  &  \\
 \vdots &  &  & \ddots &  \\
sequence_{i+m} &  &  &  &  \ddots \\
\end{matrix}

Some $P_{binding}$ values are known, which in the perspective of $n \times m$ possible values where $n$ and $m$ approach infinity, coverage is sparse.

This type of problem has been addressed in recommender systems, which in the context of streaming services translates to a matrix of $n$ users and $m$ peices of content.
Known values are likes and engagement metrics and are similarly sparse, and blanks can be filled with the probability of a successful recommendation.

Machine learning models can be trained to predict the unknown values based on a numerical representation of the user and content.
The prediction can be cast as a classification problem.
To overcome the lack of negative data points, presumed negative data can be generated by sampling a random user: content pair, which should be treated with caution.

In this work, a machine learning model classifies pairs of protein sequence and small molecules as binding or not.
Negative samples are generated by randomly sampling a sequence and small molecule, which given the vastness of sequence and chemical space may be reasonable in a large number of cases, though this assumption is treated with caution.
A Binary Cross Entropy loss function is employed here where true positives and synthesized negatives are weighted evenly.



!!! info
	Binary Cross Entropy (BCE) is a common machine learning loss function where binary classification is concerned.

	From torch docs:

	$$
	    \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
	    l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],
	$$

	$$
	    \ell(x, y) = \begin{cases}
		\operatorname{mean}(L), & \text{if reduction} = \text{mean;}\\
		\operatorname{sum}(L),  & \text{if reduction} = \text{sum.}
	    \end{cases}
	$$


### Transfer Learning

Transfer learning is a phenomenon where a model trained on one task, can be re-trained on a different domain-related task  sample-efficiently compared to an untrained model.
For example an object detection model trained from from images of vehicles can transfer efficiently to identification of cell phenotypes from microscopy images.
This is because some learned features generalize well enough to be useful in other tasks, reducing the number is samples required to reach a baseline performance level.

In the domain of protein sequence-based machine learning, thoroughly pre-trained models are available for generating a neural embedding of a given protein that can improve sample efficiency in downstream learning tasks.
Generally, these models are large attention-based models trained *unsupervised* on a large corpus of protein sequences, like the *TrEMBL* collection of *Uniprot*.
In this case, unsupervised training often entails reconstruction of a distorted or masked protein sequence and is run on hardware far beyond the budget of this project.

#### The TAPE Benchmark

Tasks Assessing Protein Embeddings (TAPE)[@tape2019] is a benchmark for comparing numerical representations of protein sequence (learned or otherwise) on a set of biological learning tasks from different domains of protein science. 
It currently contains five tasks:

1. **Secondary Structure Prediction Task:**
2. **Structural Contact Prediction Task:**
3. **Remote Homology Detection:**
4. **Flourescent Protein Landscape Prediction:**
5. **Protein Stability Landscape Prediction:**

Tasks 4 and 5 are most applicable to protein eningeering, since they involve metric prediction from a set of largely similar protein sequences.
The leaderboards for performance on these two tasks as of 5 Jun 2022 are:

##### Fluorescence

| Ranking | Model | Spearman's rho |
|:-:|:-:|:-:|
| 1. | Transformer | 0.68 |
| 2. | LSTM | 0.67 |
| 2. | Unirep | 0.67 |
| 4. | Bepler | 0.33 |
| 5. | ResNet | 0.21 |
| 6. | One Hot | 0.14 |

##### Stability

| Ranking | Model | Spearman's rho |
|:-:|:-:|:-:|
| 1. | Transformer | 0.73 |
| 1. | Unirep | 0.73 |
| 1. | ResNet | 0.73 |
| 4. | LSTM | 0.69 |
| 5. | Bepler | 0.64 |
| 6. | One Hot | 0.19 |

#### Facebook AI Research - Evolutionary-Scale Modelling
[@rives2021biological]

## Data

Small molecules are represented as SMILES codes in the dataset, which are parsed using `rdkit` and then into 2048 bit fingerprint vectors using the `RdkitFingerprint`.
Molecular fingerprints are generated by hashing functions based on an input molecule such that similar molecules are assigned similar fingerprints, which makes them useful in featurizing small molecules for machine learning tasks.
The fingerprints are represented as a $b \times 2048$ tensor where $b$ is batch size.

Sequences are represented in the dataset as strings where each character $c_i$ is a single letter amino acid code:

$$c_i \subset{ACDEFGHIKLMNPQRSTVWY}$$

Roughly, characters are encoded as tensors of the integers that index their position in the list $\begin{bmatrix} ACDEFGHIKLMNPQRSTVWY\end{bmatrix}$, with extra positions to represent null values, start of frame and end of frame characters.

### Pre-Training Data

### Training Data

## Model Architecture

The model aims to predict:
$$
P_{binding} = fn(sequence, smiles)
$$
Where $fn$ is a model that takes an input of a protein $sequence$ and a prospective ligands' SMILES code - $smiles$ and outputs $P_{binding}$ - an estimate of the probability that the given $sequence$ and $smiles$ bind to one another.

Given their prior success in chemical and protein sequence learning, a neural network model was chosen to build  $fn$.
The network can be split into three parts:


- **Sequence Embedding:** For a given protein $sequence$, output a tensor encoding a neural embedding $z_{sequence}$.
- **Chemical Embedding:** For a given chemical $smiles$ encoding, outputs an embedding $z_{smiles}$.
- **Prediction Head:** For the embeddings $z_{smiles}$ and $z_{sequence}$, output a prediction $P_{binding}$.

```mermaid
graph TD;

f1(SMILES) --parse--> f2(Fingerprint) ;
f2 --> f3(Feed Forward) ;
f3 --> f4(Combined Vector) ;

s1(Sequence) --parse--> s2(Tokens); 
s2 --> s3(Esm) ;
s3 --> s4(Pooling) ;
s4 --> f4;
f4 --> h1(Prediction Head) ; 
h1 --> o1(Output Prediction) ;
```

### Sequence Embedding

Sequences were embedded using the pre-trained `esm1_t6_43M_UR50S` model [@rives2021biological], which receives an input of a tokenized protein sequence and outputs a tensor of size $b l d$ where $b$ is batch size, $l$ is sequence length and $d$ is 35.

Although this model is the smallest of the ESM collection, on the single *NVIDIA Quadro RTX 6000* used it still occupied most of the 24 GB of available memory and most of the processing capability, which lead to long training times and difficulty in training more than one model in parallel on the same machine.

This could be remedied, however since in the complete `o3f.csv` dataset and the screening dataset there are 2947 unique sequences, so it was economical to pre-compute the embeddings and save them to disk.
This resulted in a roughly 4$x$ speedup in training time and massively reduced the memory requirements, allowing several models to be trained in parallel on a single GPU.
This also saved costs significantly.

### Chemical Embedding

As mentioned, chemical SMILES were hashed into chemical fingerprints using the `rdkit` `RDKFingerprint` method as a means of representation, yielding a 2048-bit vector for each compound.
The vectors were converted to tensors and served as an input to a residual neural network that output an embedding that would later be used to form a combined representation of both compound and sequence for binding likelihood prediction.

### Prediction Head

The combined sequence and compound embeddings served as input to the prediction head, which output a single number that indicated a binding likelihood prediction for the two inputs.

Both residual neural networks and transformers were compared as prediction head architectures.
Each consisted of 2-6 stacked layers of either residual or transformer layers with a fixed hidden layer size for convenience of automated assembly.
The final layer in both cases was a single linear layer with a single output and a sigmoid function to output a number between 0 and 1 representing binding probability.

## Pre-Training, Training and Evaluation

Training was done in two stages, each with a performance evaluation, during which several models with varying architectures and hyper-parameters were trained and compared.
All training was done on a *Linode* `g1-gpu-rtx6000-1` which cost \$1.50 per hour and was equipped with the following hardware specifications:

| Item | Specifications | Number | Size | 
|------|----------------|--------|------|
| CPU  | Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz | 8 |
| RAM  | ?  | N/A | 30 GB |
| Disk | ?  | 1   | 630 GB |
| GPU  | NVIDIA Quadro RTX 6000 | 1 | 20 GB VRAM | 

1. **Pre-Training:** This was done with the larger, more general `o3f` dataset, which was randomly split into training and validation partitions, the latter of which was used sparingly to avoid model bias.
Pre-training lasted up to 64 epochs with a batch size up to 64.
For each sample, a random sequence and SMILES pair were sampled as a presumed negative sample.
The loss function used was binary cross entropy used with an Adam (Adaptive momentum) optimizer.
Loss was tracked live using the *Weights and Biases* API which was useful to evaluate models as they trained and terminate them where necessary.
Model weights were saved in each epoch and after training the model was evaluated for precision and accuracy on a subset of the training data.
The metrics gathered were: 

	- Mean binary cross entropy loss over evaluation.
	- Mean precision
	- A confusion matrix
	- A receiver operator curve (ROC)
	- A precision recall curve
	- A detection error trade-off (DET) curve

2. **Training:** This was done with the manually annotated screening dataset.
An issue with the data was the class imbalance in that there were very few positive examples relative to negative.
This was addressed by using *Synthetic Minority Oversampling* (SMOTE) whereby the rarer positive data were re-sampled until they number that of the negative data.
The total size of the re-sampled data was 6666 points, which were then split 3:1 into training and validation sets of size 4999 and 1667 respectively.
A model pre-trained on the larger `o3f` dataset was re-trained on this set and evaluated for performance in the same manner as with the `o3f` data, visualised in the following section.


### Evaluation

## Sequence Optimization

## Active Learning

\bib
